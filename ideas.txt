# Alan's thought process for how this whole pre-process thing will work.

One challenge will be iterating our neural network through all our data. We
cannot store all the data on ram at one time. We simply have too much data.

If there is a way to combine results of a run, then that would be ideal.

There is 120 GB of compressed images. We could even reduce these to each pixel
being 0-100 (the value).

Each image would have exactly 50176 inputs. How could we lower that?

One thing was image recognition.

Apparently there is image recognition software out there. I'll bet that we can
associate colors with different animals/objects more easily than just pixels.

https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html

So, more inputs could be what kind of object the item is.

This is a good site on convolutional neural networks.
http://colah.github.io/posts/2014-07-Conv-Nets-Modular/

As far as image classification goes, the conv neural net was able to create
edge inputs of sorts.

This site shows implementation.
http://deeplearning.net/tutorial/lenet.html

If there was a mathematical way to represent texture in a region, then that
would be a great input. The texture and value of a region, along with what type
of object is in the image.

Dividing the image up into multiple sections, then averaging the value of the
section, and comparing that average with the average of other sections.
Sections with the same average would probably be the same general color.

From a superficial standpoint, it seems that about an 80x80 square is needed to
figure out a texture. This is about 2.8 cm on screen.

Our images are 224x224, there could be:

1   224x224 overall texture value.
4   112x112 texture values.
16  56x56   texture values.
64  28x28   texture values.
256 14x14   texture values.

That adds up to 85 extra nodes.
Or 341 including the 14x14 texture values.

There is also the question of how to normalize the grid squares. Do we want
mean, median, or mode.

We could even be getting rid of most of the 50176 nodes and only have these
nodes. But, that may be what convolutional neural networks were about anyway.

Another thought. Images are more closely related to itself than to other
images. As far as color goes that is. So having an image learn from itself
could be very beneficial.

Here is a question, constructing a grid of offsets for the texture analysis
could be a bit disturbing, because the grid may not be aligned to the image.
Some sort of edge detection algorithm could possible fix this.

http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_canny/py_canny.html

The link above is a python implementation of edge detection. If the grid
squares could realize that it was within the edges, it would probably react
differently than if it weren't within an edge.

Also, does the entire image depend on the color of a single pixel?
	YES! But once we find that single pixel, we can copy it over to a bunch of
	other spots in the image. The spots with similar textures will have similar
	colors. So, out of all the grid squares with similar textures, whichever
	color looks the most likely, apply that general color to all the grid
	squares in the image.

	Grid square colors are determined by past results.

Of course, the image classification will also play a part in this, if the image
is a panda, then colors are more likely to be white. If the image is the ocean,
then the texture/overall image will most likely be blue.

We could also get the overall saturation and hue of a colored image, using the
grid idea from above. And another useful preprocessing idea would be to convert
the gray-scale image into only 4 colors. That way the resulting average colors
would be more distinct or something.

This is a big problem, and there are so many ideas. The main thing to worry
about will be figuring out our inputs.
